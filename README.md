# ğŸ§  German NER Product Info Extraction

This project is a full Named Entity Recognition (NER) pipeline for extracting structured product information (e.g. brand, storage, color) from messy German e-commerce texts. It includes rule-based + machine learning (hybrid) entity extraction, training, evaluation, logging, and a FastAPI service for serving predictions.

---

## ğŸ“ Project Structure

```bash
ner_project/
â”œâ”€â”€ data/                   # Cleaned and processed text data
â”‚   â”œâ”€â”€ ds_ner_test_case.csv
â”‚   â”œâ”€â”€ train_data.spacy
â”‚   â”œâ”€â”€ dev_data.spacy
â”‚   â”œâ”€â”€ matcher.pkl
â”‚   â”œâ”€â”€ ner_training_data.pkl
â”œâ”€â”€ output/                 # Trained model artifacts
â”œâ”€â”€ logs/                   # Evaluation logs
â”œâ”€â”€ plots/                  # Confusion matrix, entity distribution
â”œâ”€â”€ main.py                 # Data cleaning, rule-based tagging, export to .spacy
â”œâ”€â”€ train.py                # Model training with spaCy config
â”œâ”€â”€ evaluate.py             # Evaluation with metrics + confusion matrix
â”œâ”€â”€ apifast.py              # FastAPI hybrid service
â”œâ”€â”€ test_api.py             # Unit tests for API
â”œâ”€â”€ test_extract.py         # Unit tests for matcher-based extraction
â”œâ”€â”€ distribution.py         # Visualize entity frequency in training data
â”œâ”€â”€ config.cfg              # spaCy configuration file
â”œâ”€â”€ Dockerfile              # Docker image setup
â”œâ”€â”€ docker-compose.yml      # Docker orchestration for API
â”œâ”€â”€ requirements.txt        # All dependencies
â””â”€â”€ README.md               # ğŸ“– Project usage guide (you're reading this!)
```

---

## ğŸš€ Quickstart (Development)

### 1. Install Requirements

```bash
pip install -r requirements.txt
```

### 2. Clean + Prepare Data

```bash
python main.py
```

Generates:

* `ner_training_data.pkl`
* `train_data.spacy`, `dev_data.spacy`
* `matcher.pkl`

### 3. Visualize Distribution

```bash
python distribution.py
```

Creates bar plots of frequency of labels (e.g. top brands, storage, colors).
This section has a deeper look at the data used for train. It can help us to see how labels (brands, storage, colors) were distributed and which label is more and less frequent. It is useful also not to allow the model be overfitted.

### 4. Train Model

```bash
python train.py
```

Outputs model to `output/model-best`.

**Training Configuration:**

* Language: `de` (German)
* Model: spaCy's `ner` component with default architecture
* Batch size: `128`
* Dropout: `0.1`
* Max epochs: `30`
* Eval frequency: `50`
* Config file used: `config.cfg`

### 5. Evaluate Model

```bash
python evaluate.py
```

Logs:

* Precision, Recall, F1 Score
* Confusion Matrix (in `plots/confusion_matrix.png`)
* Logs in `logs/evaluation.log`

---

## ğŸ§ª Unit Testing

Run API and extraction tests:

```bash
python test_api.py
python test_extract.py
```

---

## ğŸ“¦ Run FastAPI Server

```bash
uvicorn apifast:app --reload
```

Then open: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

Example request:

```json
{
  "text": "acer 250-G8 Laptop mit 1 TB superschnell SpeicherkapazitÃ¤t in silberene"
}
```

---

## ğŸ³ Docker Deployment

### Build the image:

```bash
docker build -t ner-app .
```

### Run container:

```bash
docker run -p 8000:8000 ner-app
```

Or use docker-compose:

```bash
docker-compose up --build
```



## âœ… Deliverable Features

* [x] Entity cleaning, tagging, saving
* [x] Hybrid matcher + ML model
* [x] Balanced train/dev splits
* [x] Visualizations of entity distribution
* [x] Evaluation with metrics + confusion matrix
* [x] Logging to `logs/`
* [x] FastAPI service
* [x] Dockerized
* [x] Unit tests (API + Extractor)

---

## ğŸ“Œ Notes for Reviewers

* You can run **everything from scratch** using only the code in this repo.
* Output folders will be auto-created where needed.
* The API will return predictions even if only rule-based matcher is available (fallback logic).
* Model configuration and training metadata can be found in `config.cfg`

Feel free to explore and test!

---

Made with â¤ï¸ and Python.

